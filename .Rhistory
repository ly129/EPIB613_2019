# Get author list with indicies
aff.style(first = data$FIRST.NAME,last = data$LAST.NAME, degree = data$DEGREES, index = data$index, latex = T)
# change the inputs and the cat() commands according to your need
# ?cat
# If superscripted indices are needed
# set latex = TRUE and copy-paste the output to R markdown and knit to MS Word.
aff.style <- function(first, last, degree, index, latex = FALSE) {
first <- as.character(first)
last <- as.character(last)
degree <- as.character(degree)
index <- as.character(index)
for (i in 1:length(first)) {
if (!latex) {
cat(first[i], " ", last[i], ", ", degree[i], index[i], "; ", sep = "" )
} else {
cat(first[i], " ", last[i], ", ", degree[i], "^", index[i], "^; ", sep = "" )
}
}
}
# Get author list with indicies
aff.style(first = data$FIRST.NAME,last = data$LAST.NAME, degree = data$DEGREES, index = data$index, latex = T)
# Get affiliation list with superscripted indicies
# Knit in R Markdown
for (i in 1:result$count) {
cat("^", i, "^ ", result$affilliations[i], "\n", sep = "")
}
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
authors
data(authors)
load(authors)
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
authors
data(authors)
library("survival", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
cancer
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
data
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
authors
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
result <- AutoAff(authors, c("aff1", "aff2", "aff3", "aff4", "aff5", "aff6"))
View(result)
result
template <- "f m l, d^i^"
AffStyle(X = authors, first = "First", middle = "Middle", last = "Last", degree = "Degree", index = result$index, template = template)
# Load data, na.strings = c("", NA) set empty entries to NA
data <- read.csv("~/Documents/RandomTests/AutoAff_backup/Book1.csv") # Give full path name
authors
library("AutoAff", lib.loc="/Library/Frameworks/R.framework/Versions/3.6/Resources/library")
authors
data(authors)
authors
View(authors)
View(data)
results <- AutoAff(data, c("AFFILIATION.1", "AFFILIATION.2", "AFFILIATION.3"))
View(results)
results
template <- "n, d^i^;"
AffStyle(X=data, name = "FULL.NAME", degree = "DEGREES", index = results$index, template=template)
results$rmd
library(codebook)
codebook_browser()
library(codebook)
detach("package:codebook", unload = TRUE)
remove.packages("codebook")
devtools::install_github("rubenarslan/codebook")
library(codebook)
testdf <- data.frame(bfi_neuro_1 = rnorm(20), bfi_neuro_2 = rnorm(20),
bfi_neuro_3R = rnorm(20), age = rpois(20, 30))
item_names <- c('bfi_neuro_1', 'bfi_neuro_2', 'bfi_neuro_3R')
testdf$bfi_neuro <- aggregate_and_document_scale(testdf[, item_names])
testdf$bfi_neuro
old_base_dir <- knitr::opts_knit$get("base.dir")
knitr::opts_knit$set(base.dir = tempdir())
on.exit(knitr::opts_knit$set(base.dir = old_base_dir))
data("bfi")
bfi <- bfi[, c("BFIK_open_1", "BFIK_open_1")]
md <- codebook(bfi, survey_repetition = "single", metadata_table = FALSE)
md
remove.packages("ggplot2")
remove.packages("ggridges")
remove.packages("GGally")
remove.packages("abind")
remove.packages("codebook")
remove.packages("arm")
remove.packages("BiasedUrn")
remove.packages("car")
remove.packages("carData")
remove.packages("clipr")
remove.packages("colorspace")
remove.packages("crosstalk")
remove.packages("curl")
remove.packages("data.table")
remove.packages("data.tree")
remove.packages("DiagrammeR")
remove.packages("digest")
remove.packages("diptest")
remove.packages("downloader")
remove.packages("DT")
remove.packages("forcats")
remove.packages("future")
remove.packages("ggrepel")
remove.packages("globals")
remove.packages("GPArotation")
remove.packages("gridExtra")
remove.packages("gtable")
remove.packages("haven")
remove.packages("htmlwidgets")
remove.packages("igraph")
remove.packages("influenceR")
remove.packages("labeling")
remove.packages("labelled")
remove.packages("lavaan")
remove.packages("likert")
remove.packages("listenv")
remove.packages("lubridate")
remove.packages("maptools")
remove.packages("miniUI")
remove.packages("minpack.lm")
remove.packages("rio")
remove.packages("pbdZMQ")
remove.packages("MBESS")
remove.packages("tidyr")
remove.packages("tidyselect")
remove.packages("tinytex")
remove.packages("codetools")
remove.packages("reshape")
remove.packages("rgexf")
remove.packages("reshape2")
library(parallel)
Rcpp::sourceCpp('Desktop/Untitled.cpp')
library(TDboost)
library(TDkernel)
library(MASS)
library(tweedie)
library(statmod)
library(mgcv)
n.sim <- 20
sim1 <- sim2 <- sim3 <- sim4 <- rep(NA, n.sim)
# set.seed(20190815)
rand_tweedie<- function(mu,...) {
Y <- rtweedie(1, mu = mu,...)
Y
}
phi <- 0.5
rho <- 1.5
P <- 1
N <- 1000
for (i in 1:n.sim) {
X1 <- matrix(runif(N * P, 0, 1), N, P)
# X2 <- matrix(runif(N * P, 0, 1), N, P)
Fx <- 0.5 * (X1 > 0.5)
mu <- exp(Fx)
Y = sapply(mu, rand_tweedie, xi = rho, phi = phi)
truef <- Fx[(N/2 + 1):N]
dat = data.frame(cbind(Y, X1))
colnames(dat) <- c("Y","X1")
train_dat <- dat[1:(N/2),]
test_dat <- dat[(N/2 + 1):N,]
m1 = gam(Y~s(X1),data = train_dat, family=Tweedie(p=rho, link = "log"))
pred_f1 = predict.gam(m1, newdata = test_dat)
m2 <- TDboost(Y~X1,    # formula
data = train_dat,
distribution = list(name="EDM",alpha=rho), # poisson, coxph, and quantile available
shrinkage = 0.001,               # shrinkage or learning rate,
n.trees = 10000,              # 0.001 to 0.1 usually work
interaction.depth = 1,         # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 1,            # subsampling fraction, 0.5 is probably best
n.minobsinnode = 20,		 # minimum total weight needed in each node
keep.data = TRUE,              # keep a copy of the dataset with the object
cv.folds = 5,
verbose = FALSE)                # print out progress
best.iter <- TDboost.perf(m2,method="cv", plot.it = TRUE)
pred_f2 <- predict.TDboost(m2, test_dat, best.iter)
m3 <- glm(Y~X1,    # formula
data = train_dat,
family = tweedie(link.power=0,var.power=rho))
pred_f3 <- predict(m3, test_dat)
kern <- rbfdot(sigma = 0.01)
LamReg <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
tdkern <- TD_cv(x = as.matrix(train_dat[, c("X1")]), z = train_dat[, "Y"], kern = kern,
lambda = LamReg, nfolds = 5,
phi = phi, rho = rho,
ftol = 1e-8, quiet = TRUE)
m4 <- TD_estimate(x = as.matrix(train_dat[, c("X1")]), z = as.vector(train_dat[, "Y"]), kern = kern,
LamReg = tdkern$Best_lambda, phi = phi, rho = rho,
ftol = 1e-8, quiet = TRUE, method = "fortran")
pred_f4 <- TD_predict(x = as.matrix(test_dat[, c("X1")]), model = m4, lambda = tdkern$Best_lambda)
pred_f4 <- log(as.vector(pred_f4$prediction))
sim1[i] <- mean(abs(pred_f1 - truef))
sim2[i] <- mean(abs(pred_f2 - truef))
sim3[i] <- mean(abs(pred_f3 - truef))
sim4[i] <- mean(abs(pred_f4 - truef))
cat("No of simulation:", i, "\n")
}
mean(sim1); mean(sim2); mean(sim3); mean(sim4)
library(TDboost)
library(TDkernel)
library(scatterplot3d)
library(MASS)
library(tweedie)
library(statmod)
library(mgcv)
n.sim <- 20
sim1 <- sim2 <- sim3 <- sim4 <- rep(NA, n.sim)
# set.seed(20190815)
rand_tweedie<- function(mu,...) {
Y <- rtweedie(1, mu = mu,...)
Y
}
phi <- 0.5
rho <- 1.5
P <- 1
N <- 2000
for (i in 1:n.sim) {
X1 <- matrix(runif(N * P, 0, 1), N, P)
# X2 <- matrix(runif(N * P, 0, 1), N, P)
Fx <- 0.5 * (X1 > 0.5)
mu <- exp(Fx)
Y = sapply(mu, rand_tweedie, xi = rho, phi = phi)
truef <- Fx[(N/2 + 1):N]
dat = data.frame(cbind(Y, X1))
colnames(dat) <- c("Y","X1")
train_dat <- dat[1:(N/2),]
test_dat <- dat[(N/2 + 1):N,]
m1 = gam(Y~s(X1),data = train_dat, family=Tweedie(p=rho, link = "log"))
pred_f1 = predict.gam(m1, newdata = test_dat)
m2 <- TDboost(Y~X1,    # formula
data = train_dat,
distribution = list(name="EDM",alpha=rho), # poisson, coxph, and quantile available
shrinkage = 0.001,               # shrinkage or learning rate,
n.trees = 10000,              # 0.001 to 0.1 usually work
interaction.depth = 1,         # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 1,            # subsampling fraction, 0.5 is probably best
n.minobsinnode = 20,		 # minimum total weight needed in each node
keep.data = TRUE,              # keep a copy of the dataset with the object
cv.folds = 5,
verbose = FALSE)                # print out progress
best.iter <- TDboost.perf(m2,method="cv", plot.it = TRUE)
pred_f2 <- predict.TDboost(m2, test_dat, best.iter)
m3 <- glm(Y~X1,    # formula
data = train_dat,
family = tweedie(link.power=0,var.power=rho))
pred_f3 <- predict(m3, test_dat)
kern <- rbfdot(sigma = 0.01)
LamReg <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
tdkern <- TD_cv(x = as.matrix(train_dat[, c("X1")]), z = train_dat[, "Y"], kern = kern,
lambda = LamReg, nfolds = 5,
phi = phi, rho = rho,
ftol = 1e-8, quiet = TRUE)
m4 <- TD_estimate(x = as.matrix(train_dat[, c("X1")]), z = as.vector(train_dat[, "Y"]), kern = kern,
LamReg = tdkern$Best_lambda, phi = phi, rho = rho,
ftol = 1e-8, quiet = TRUE, method = "fortran")
pred_f4 <- TD_predict(x = as.matrix(test_dat[, c("X1")]), model = m4, lambda = tdkern$Best_lambda)
pred_f4 <- log(as.vector(pred_f4$prediction))
sim1[i] <- mean(abs(pred_f1 - truef))
sim2[i] <- mean(abs(pred_f2 - truef))
sim3[i] <- mean(abs(pred_f3 - truef))
sim4[i] <- mean(abs(pred_f4 - truef))
cat("No of simulation:", i, "\n")
}
mean(sim1); mean(sim2); mean(sim3); mean(sim4)
library(TDboost)
library(TDkernel)
library(MASS)
library(tweedie)
library(statmod)
library(mgcv)
n.sim <- 20
sim1 <- sim2 <- sim3 <- sim4 <- rep(NA, n.sim)
# set.seed(20190815)
rand_tweedie<- function(mu,...) {
Y <- rtweedie(1, mu = mu,...)
Y
}
h2 <- function(x) {
exp( -5 * (1-x[,1])^2 + (x[,2])^2 ) + exp( -5 * (x[,1])^2 + (1-x[,2])^2 )
}
phi <- 0.5
rho <- 1.5
P <- 2
N <- 500
for (i in 1:n.sim) {
T1 = seq(0,1,0.03)
T2 = seq(0,1,0.03)
TT = expand.grid(T1,T2)
colnames(TT) <- c("V1","V2")
test_dat = data.frame(TT)
nn = nrow(TT)
# generate train X data
XX <- matrix(runif(N * P, 0, 1), N, P)
# combine two data and generate Fx
nobs <- N + nrow(TT)
X = rbind(XX, TT)
Fx <- h2(X)
scatterplot3d(x=XX[,1],y=XX[,2],z=Fx[1:N],xlim=c(-2,2),ylim=c(-2,2), pch = ".", angle=120, cex.symbols=0.8)
mu <- exp(Fx[1:N])
Y = sapply(mu, rand_tweedie, xi = rho, phi = phi)
truef = Fx[(N+1):nobs]
train_dat = data.frame(cbind(Y, XX))
colnames(train_dat) <- c("train_y", "V1", "V2")
m1 = gam(train_y~s(V1)+s(V2),data = train_dat, family=Tweedie(p=rho, link = "log"))
pred_f1 = predict.gam(m1, newdata = test_dat)
scatterplot3d(x=XX[,1],y=XX[,2],z=pred_f1[1:N],xlim=c(-2,2),ylim=c(-2,2), pch = ".", angle=150, cex.symbols=0.8)
m2 <- TDboost(train_y~V1+V2,    # formula
data = train_dat,
distribution = list(name="EDM", alpha=rho), # poisson, coxph, and quantile available
shrinkage = 0.001,               # shrinkage or learning rate,
n.trees = 30000,              # 0.001 to 0.1 usually work
interaction.depth = 2,         # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 1,            # subsampling fraction, 0.5 is probably best
n.minobsinnode = 20,		 # minimum total weight needed in each node
keep.data = TRUE,              # keep a copy of the dataset with the object
cv.folds = 5,
verbose = FALSE)                # print out progress
best.iter <- TDboost.perf(m2, method="cv", plot.it = TRUE)
pred_f2 <- predict.TDboost(m2, test_dat, best.iter, type = "link")
scatterplot3d(x=XX[,1],y=XX[,2],z=pred_f2[1:N],xlim=c(-2,2),ylim=c(-2,2), pch = ".", angle=50, cex.symbols=0.8)
m3 <- glm(train_y~V1+V2,    # formula
data = train_dat,
family = tweedie(link.power=0, var.power=rho))
pred_f3 <- predict(m3, test_dat)
scatterplot3d(x=XX[,1],y=XX[,2],z=pred_f3[1:N],xlim=c(-2,2),ylim=c(-2,2), pch = ".", angle=150, cex.symbols=0.8)
kern <- rbfdot(sigma = 0.01)
LamReg <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
tdkern <- TD_cv(x = as.matrix(train_dat[, c("V1", "V2")]), z = train_dat[, "train_y"], kern = kern,
lambda = LamReg, nfolds = 5,
phi = phi, rho = rho,
ftol = 1e-10, quiet = TRUE)
m4 <- TD_estimate(x = as.matrix(train_dat[, c("V1", "V2")]), z = as.vector(train_dat[, "train_y"]), kern = kern,
LamReg = tdkern$Best_lambda, phi = phi, rho = rho,
ftol = 1e-10, quiet = TRUE, method = "fortran")
pred_f4 <- TD_predict(x = as.matrix(test_dat[, c("V1", "V2")]), model = m4, lambda = tdkern$Best_lambda)
pred_f4 <- log(as.vector(pred_f4$prediction))
scatterplot3d(x=XX[,1],y=XX[,2],z=pred_f4[1:N],xlim=c(-2,2),ylim=c(-2,2), pch = ".", angle=130, cex.symbols=0.8)
sim1[i] <- mean(abs(pred_f1 - truef))
sim2[i] <- mean(abs(pred_f2 - truef))
sim3[i] <- mean(abs(pred_f3 - truef))
sim4[i] <- mean(abs(pred_f4 - truef))
cat("No of simulation:", i, "\n")
}
mean(sim1); mean(sim2); mean(sim3); mean(sim4)
range(1:5)
c(1,5)
library(TDboost)
library(TDkernel)
# library(scatterplot3d)
library(MASS)
library(tweedie)
library(statmod)
library(mgcv)
n.sim <- 10
sim1 <- sim2 <- sim3 <- sim4 <- rep(NA, n.sim)
# set.seed(20190815)
rand_tweedie<- function(mu,...) {
Y <- rtweedie(1, mu = mu,...)
Y
}
phi <- 0.5
rho <- 1.5
P <- 1
N <- 800
t0 <- proc.time()
for (i in 1:n.sim) {
X1 <- matrix(runif(N * P, 0, 1), N, P)
# X2 <- matrix(runif(N * P, 0, 1), N, P)
Fx <- 0.5 * (X1 > 0.5)
mu <- exp(Fx)
Y = sapply(mu, rand_tweedie, xi = rho, phi = phi)
truef <- Fx[(N/2 + 1):N]
dat = data.frame(cbind(Y, X1))
colnames(dat) <- c("Y","X1")
train_dat <- dat[1:(N/2),]
test_dat <- dat[(N/2 + 1):N,]
m1 = gam(Y~s(X1), data = train_dat, family=Tweedie(p=rho, link = "log"))
pred_f1 = predict.gam(m1, newdata = test_dat)
m2 <- TDboost(Y~X1,    # formula
data = train_dat,
distribution = list(name="EDM",alpha=rho), # poisson, coxph, and quantile available
shrinkage = 0.001,               # shrinkage or learning rate,
n.trees = 10000,              # 0.001 to 0.1 usually work
interaction.depth = 1,         # 1: additive model, 2: two-way interactions, etc.
bag.fraction = 1,            # subsampling fraction, 0.5 is probably best
n.minobsinnode = 20,		 # minimum total weight needed in each node
keep.data = TRUE,              # keep a copy of the dataset with the object
cv.folds = 5,
verbose = FALSE)                # print out progress
best.iter <- TDboost.perf(m2, method="cv", plot.it = F)
pred_f2 <- predict.TDboost(m2, test_dat, best.iter, type = "link")
m3 <- glm(Y~X1,    # formula
data = train_dat,
family = tweedie(link.power=0, var.power=rho))
pred_f3 <- predict(m3, test_dat)
# kern <- rbfdot(sigma = 0.01)
# LamReg <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
# tdkern <- TD_cv(x = as.matrix(train_dat[, c("X1")]), z = train_dat[, "Y"], kern = rbfdot(sigma = 1),
#                 lambda = LamReg, nfolds = 5,
#                 phi = phi, rho = rho,
#                 ftol = 1e-10, quiet = TRUE)
tdkern <- TD_cv2d(x = as.matrix(train_dat[, c("X1")]), z = train_dat[, "Y"], kernfunc = rbfdot,
lambda = c(0.0001, 10),
delta = c(0.0001, 10),
n = 25, nfolds = 5, ftol = 1e-8,
phi = phi, rho = rho, quiet = T)
m4 <- TD_estimate(x = as.matrix(train_dat[, c("X1")]), z = as.vector(train_dat[, "Y"]),
kern = rbfdot(tdkern$Best_sigma),
LamReg = tdkern$Best_lambda, phi = phi, rho = rho,
ftol = 1e-10, quiet = TRUE, method = "fortran")
pred_f4 <- TD_predict(x = as.matrix(test_dat[, c("X1")]), model = m4, lambda = tdkern$Best_lambda)
pred_f4 <- log(as.vector(pred_f4$prediction))
sim1[i] <- mean(abs(pred_f1 - truef))
sim2[i] <- mean(abs(pred_f2 - truef))
sim3[i] <- mean(abs(pred_f3 - truef))
sim4[i] <- mean(abs(pred_f4 - truef))
cat("No of simulation:", i, "\n")
cat("lambda:", tdkern$Best_lambda, ", sigma:", tdkern$Best_sigma, "\n")
hx = 0.5 * (X1[401:800] > 0.5)
par(mfrow = c(1,5))
limit = range(c(pred_f1, pred_f2, pred_f3, pred_f4))
plot(y = hx, x = as.vector(X1[401:800, ]),
xlab = expression(paste(x)), ylab = expression(paste(F(x))),
ylim = limit,
main = "(a) TRUE", pch = ".")
plot(y = pred_f1, x = as.vector(X1[401:800, ]),
xlab = expression(paste(x)), ylab = expression(paste(F(x))),
ylim = limit,
main = "(b) MGCV", pch = ".")
plot(y = pred_f2, x = as.vector(X1[401:800, ]),
xlab = expression(paste(x)), ylab = expression(paste(F(x))),
ylim = limit,
main = "(c) TDboost", pch = ".")
plot(y = pred_f3, x = as.vector(X1[401:800, ]),
xlab = expression(paste(x)), ylab = expression(paste(F(x))),
ylim = limit,
main = "(d) TGLM", pch = ".")
plot(y = pred_f4, x = as.vector(X1[401:800, ]),
xlab = expression(paste(x)), ylab = expression(paste(F(x))),
ylim = limit,
main = "(e) TDkernel", pch = ".")
}
proc.time() - t0
mean(sim1); mean(sim2); mean(sim3); mean(sim4)
install.packages("tinytex")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
cars
data(cars)
cars
head(cars)
View(cars)
knit_with_parameters('~/Desktop/Teaching/MATH533/2019/RMD_tutorial.Rmd')
unlink('Desktop/Teaching/MATH533/2019/RMD_tutorial_cache', recursive = TRUE)
install.packages("reticulate")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
table(cars)
summary(cars)
table(cars[1:6])
summary(cars)
table(cars[1:6,])
library(knitr)
summary(cars)
kable(summary(cars))
library(knitr)
summary(cars)
kable(summary(cars), format = "markdown", align = c('l','l'), caption = "Summary of Cars")
table(mtcars[, c("gear", "cyl")])
head(mtcars)
fit <- lm(hp~cyl+gear, data = mtcars)
summary(fit)
str(summary(fit))
mtcars
print(mtcars)
lapply(mtcars, typeof)
sapply(mtcars, typeof)
sapply(mtcars, class)
class(mtcars)
plot(mtcars)
plot(mtcars[,1:3])
mat <- matrix(1:12, nrow = 3)
mat
as.factor(mat)
mat <- as.factor(mat)
mat
sapply(mat, as.factor)
mtcars
sapply(mtcars, as.factor)
sapply(mtcars, typeof)
mtcars.fac <- sapply(mtcars, as.factor)
sapply(mtcars.fac, typeof)
sapply(mtcars.fac, class)
apply(mtcars, MARGIN = 2, FUN = as.factor)
mtcars.fac <- apply(mtcars, MARGIN = 2, FUN = as.factor)
class(mtcars.fac$mpg)
View(mtcars.fac)
class(mtcars.fac)
class(mtcars.fac[,1])
class(as.factor(1:12))
as.factor(mtcars$mpg)
x <- 5
setwd("~/Desktop/Teaching/EPIB613/2019/EPIB613_2019")
plot(pressure)
plot(cars)
install.packages("ggplot2")
library(ggplot2)
1+1
1+5
sqrt(10)
y <- sqrt(10)
